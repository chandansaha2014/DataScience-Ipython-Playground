{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for [Quora question](https://www.quora.com/unanswered/In-scikit-learn-what-is-the-difference-between-SGDClassifer-with-log-loss-and-logistic-regression) , \n",
    "\n",
    "to understand difference between Logisting regression and SGD classifier in sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of SGD Classifier and Logisting regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of LogisticRegression: 0.9533333333333334\n",
      "Accuracy score of SGDClassifier: 0.6533333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Note that the iris dataset is available in sklearn by default.\n",
    "# This data is also conveniently preprocessed.\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "Y = iris[\"target\"]\n",
    "\n",
    "numFolds = 10\n",
    "kf = KFold(n_splits = numFolds, shuffle=True)\n",
    "\n",
    "# These are \"Class objects\". For each Class, find the AUC through\n",
    "# 10 fold cross validation.\n",
    "Models = [LogisticRegression, SGDClassifier]\n",
    "params = [{}, {\"loss\": \"log\", \"penalty\": \"l2\"}]\n",
    "for param, Model in zip(params, Models):\n",
    "    total = 0\n",
    "    for train_indices, test_indices in kf.split(X):\n",
    "\n",
    "        train_X = X[train_indices, :]; train_Y = Y[train_indices]\n",
    "        test_X = X[test_indices, :]; test_Y = Y[test_indices]\n",
    "\n",
    "        reg = Model(**param)\n",
    "        reg.fit(train_X, train_Y)\n",
    "        predictions = reg.predict(test_X)\n",
    "        total += accuracy_score(test_Y, predictions)\n",
    "    accuracy = total / numFolds\n",
    "    print(\"Accuracy score of {0}: {1}\".format(Model.__name__, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation : \n",
    "The two algorithms are not equivalent and will not necessarily produce same accuracy given same data. Practically you can try changing the learning rate and epochs of SGD.\n",
    "\n",
    "** These algorithms are different because logistic regression uses GD where as SGD classifier uses stochastic gradient descent. The convergence of the former will be more efficient and will yield better results. However, as the size of the data set increases, SGDC should approach the accuracy of logistic regression. The parameters for GD mean different things than the parameters for SGD, so you should try adjusting them slightly. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to get similar result in *sklearn* by changing number of iterations, \n",
    "\n",
    "The default SGDClassifier n_iter is 5 meaning you do 5 * num_rows steps in weight space. The sklearn rule of thumb is ~ 1 million steps for typical data. For your example, just set it to 1000 and it might reach tolerance first. Your accuracy is lower with SGDClassifier because it's hitting iteration limit before tolerance so you are \"early stopping\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation after changing number of iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of LogisticRegression:0.9533333333333334\n",
      "Accuracy score of SGDClassifier:0.9666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Added n_iter here\n",
    "params = [{}, {\"loss\": \"log\", \"penalty\": \"l2\", 'n_iter':1000}]\n",
    "\n",
    "for param, Model in zip(params, Models):\n",
    "    total = 0\n",
    "    for train_indices, test_indices in kf.split(X):\n",
    "        train_X = X[train_indices, :]; train_Y = Y[train_indices]\n",
    "        test_X = X[test_indices, :]; test_Y = Y[test_indices]\n",
    "        reg = Model(**param)\n",
    "        reg.fit(train_X, train_Y)\n",
    "        predictions = reg.predict(test_X)\n",
    "        total += accuracy_score(test_Y, predictions)\n",
    "\n",
    "    accuracy = total / numFolds\n",
    "    print(\"Accuracy score of {0}:{1}\".format(Model.__name__, accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
